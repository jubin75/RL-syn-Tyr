# RL-De-Novo-TYR-Inhibitor-Generation-Based-on-Lead-Compound
Molecular generation based on reference molecules and forward synthesis techniques


## RL model-driven forward structural optimization of the AI-generated TYR inhibitor AI10
Using compound AI10 as the reference molecule, an RL-driven molecular generation strategy to autonomously design target compounds were employed. Specifically, a fragment-based growth and expansion approach was applied, in which the reference molecule was decomposed into multiple query fragments. These fragments were then used to assemble sets of structurally related fragments, from which new molecules were generated by predefined reaction templates. The resulting candidates were evaluated by structural similarity and docking against the reference scaffold, enabling the algorithm to yield compounds with comparable architectures and physicochemical properties. By integrating predicted activity, drug-likeness, synthetic feasibility, and structural similarity to the lead compound AI10, 32 candidate molecules (AI10-a1 to AI10-a32) were prioritized for laboratory synthesis.

## Installation (recommended)
This repo is organized as a **repo-root project** with `Ref_based_syn` as a Python package.

- **Create conda env** (example; adjust for your platform):

```bash
conda env create -f Ref_based_syn/environment.yaml
conda activate ref_syn
```

Notes:
- **RDKit** is required for fragment decomposition / reactions / similarity.
- **Docking** (optional) requires `openbabel` + `qvina02` + a receptor file. This repo stores the receptor under `test/2y9x/2y9x_pocket.pdb` and converts it to `.pdbqt` automatically when `obabel` is available.

## Quick validation: inference pipeline (with a built-in SMILES case)
We provide a lightweight inference + validation script that follows the paragraph above:
decompose reference → retrieve similar fragments/building blocks → generate by reaction templates →
evaluate similarity + (optional) docking → apply drug-likeness + synthetic feasibility filters → rank Top-K.

Run from **repo root**:

```bash
python -m Ref_based_syn.infer --validate
```

Outputs a JSON report under `./results/` (e.g. `ref_infer_validation_*.json`) containing:
- `meta`: reference SMILES, number of query fragments, number of templates, docking enabled or not
- `top_k`: ranked candidates with similarity/drug-like/synthesizable scores (and docking score when available)

### Test assets location (important)
- **Reference/lead compound (AI10)**: `test/2y9x/compounds.smi`
- **Receptor pocket**: `test/2y9x/2y9x_pocket.pdb`

`Ref_based_syn.infer` will default to the first SMILES in `test/2y9x/compounds.smi` if `--ref_smiles` is not provided.

## Reviewer minimal reproducibility (hard checklist)
This section is intended for **peer-reviewers** who want a minimal end-to-end run.

### Supported platforms for the minimal run
- **Linux/macOS**: the minimal inference validation does **not** require GPU.
- **Docking note**: the shipped `Ref_based_syn/qvina02` binary may be platform-specific. If docking cannot run on your OS, the validation still runs (docking will be skipped unless explicitly enabled and available).

### Step 0 — clone and enter repo

```bash
git clone <REPO_URL>
cd RL-syn-Tyr
```

### Step 1 — create the environment

```bash
conda env create -f Ref_based_syn/environment.yaml
conda activate ref_syn
```

### Step 2 — verify test assets exist (AI10 + receptor pocket)

```bash
ls -lh test/2y9x/compounds.smi test/2y9x/2y9x_pocket.pdb
```

Expected:
- `test/2y9x/compounds.smi` contains at least **one** SMILES (first line = AI10)
- `test/2y9x/2y9x_pocket.pdb` exists

### Step 3 — run the minimal inference+validation (no docking)

```bash
python -m Ref_based_syn.infer --validate
```

Expected:
- exit code **0**
- prints a line like: `output=.../ref_infer_validation_YYYYmmddHHMMSS.json`
- the JSON contains `top_k` and `all_count` fields

### Optional — enable docking (if your environment supports it)
If you have `obabel` and a working `qvina02` for your platform:

```bash
python -m Ref_based_syn.infer --validate --do_docking
```

Expected:
- receptor `test/2y9x/2y9x_pocket.pdb` is converted to a temporary `.pdbqt` under `./tmp/` when needed
- `docking_score` appears in the output JSON for candidates

### About training reproducibility
`python -m Ref_based_syn.main` corresponds to the original TD3 training entry. Full training runs typically require
additional prepared data artifacts referenced by `SynthesisEnv` (e.g. reaction-filter outputs and embeddings).
For peer-review we recommend using the minimal validation pipeline above as the “always runnable” path.

## Training entry (original code)
If you want to train TD3 on the filtered reaction set (requires the prepared `data/*` files used by `SynthesisEnv`):

```bash
python -m Ref_based_syn.main --max_episodes 2
```

## Project wiring (repo-root base module)
`Ref_based_syn` uses the repo-root shared module `rlsyn_base` for **stable paths** (e.g. the shared `data/` directory).
This keeps `data/` at the repo root unchanged while allowing `Ref_based_syn` to be imported and executed reliably.